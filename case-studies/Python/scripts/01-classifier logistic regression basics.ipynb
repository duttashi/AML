{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Logistic regression is a method for binary classification. It works to divide points in a dataset into two distinct classes, or categories. For simplicity, let’s call them class A and class B. The model will give us the probability that a given point belongs in category B. If it is low (lower than 50%), then we classify it in category A. Otherwise, it falls in class B. It’s also important to note that logistic regression is better for this purpose than linear regression with a threshold because the threshold would have to be manually set, which is not feasible. Logistic regression will instead create a sort of S-curve (using the sigmoid function) which will also help show certainty, since the output from logistic regression is not just a one or zero. Here is the standard logistic function, note that the output is always between 0 and 1, but never reaches either of those values.\n",
    "\n",
    "<img src=\"https://i0.wp.com/www.stokastik.in/wp-content/uploads/2017/07/sigmoid.png?w=400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When to Use\n",
    "\n",
    "Logistic regression is great for situations where you need to classify between two categories. Some good examples are accepted and rejected applicants and victory or defeat in a competition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does it work?\n",
    "\n",
    "Logistic regression works using a linear combination of inputs, so multiple information sources can govern the output of the model. The parameters of the model are the weights of the various features, and represent their relative importance to the result. In the equation that follows, you should recognize the formula used in linear regression. Logistic regression is, at its base, a transformation from a linear predictor to a probability between 0 and 1.\n",
    "\n",
    "![logisticEquation](../img/WikiLogisticEQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code\n",
    "\n",
    "In the example, `scikit-learn` and `numpy` are used to train a simple logistic regression model. The model is basic, but extensible. With logistic regression, more features could be added to the data set seamlessly, simply as a column in the 2D arrays.\n",
    "\n",
    "The code creates a 2D array representing the training input, in this case it is 1000 x 1, since there are 1000 samples and 1 feature. These inputs are scores out of 1000. A training output array is also created, with the classification of 1 for pass and 0 for fail, based on a threshold. Then, scikit-learn’s `LogisticRegression` class is used to fit a logistic regression classifier to the data. After that, the next step is to test for accuracy with a different data set. So, we create another 100 random samples to test against, and predict against them using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the required libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a random dataset which includes random scores from 0 to 1000.\n",
    "x = np.array([ random.randint(0,1000) for i in range(0,1000) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines the classification for the training data.\n",
    "def true_classifier(i):\n",
    "    if i >= 700:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model will expect a 2D array, so we must reshape\n",
    "#For the model, the 2D array must have rows equal to the number of samples,\n",
    "#and columns equal to the number of features.\n",
    "#For this example, we have 1000 samples and 1 feature.\n",
    "x = x.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each point, y is a pass/fail for the grade. The simple threshold is arbitrary,\n",
    "#and can be changed as you would like. Classes are 1 for success and 0 for failure\n",
    "y = [ true_classifier(x[i][0]) for i in range(0,1000) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Again, we need a numpy array, so we convert.\n",
    "y = np.array(y)\n",
    "\n",
    "#Our goal will be to train a logistic regression model to do pass/fail to the same threshold.\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "#The fit method actually fits the model to our training data\n",
    "model = model.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create 100 random samples to try against our model as test data\n",
    "samples = [random.randint(0,1000) for i in range(0,100)]\n",
    "#Once again, we need a 2d Numpy array\n",
    "samples = np.array(samples)\n",
    "samples = samples.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we use our model against the samples.  output is the probability, and _class is the class.\n",
    "_class = model.predict(samples)\n",
    "proba = model.predict_proba(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_accurate = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[541]: Class 0, probability [0.89636615 0.10363385]\n",
      "[712]: Class 1, probability [0.42488221 0.57511779]\n",
      "[491]: Class 0, probability [0.9466904 0.0533096]\n",
      "[662]: Class 0, probability [0.60267125 0.39732875]\n",
      "[913]: Class 1, probability [0.03936819 0.96063181]\n",
      "[67]: Class 0, probability [9.99873750e-01 1.26250097e-04]\n",
      "[276]: Class 0, probability [0.99745265 0.00254735]\n",
      "[474]: Class 0, probability [0.95776858 0.04223142]\n",
      "[432]: Class 0, probability [0.97647104 0.02352896]\n",
      "[572]: Class 0, probability [0.84702752 0.15297248]\n",
      "[446]: Class 0, probability [0.97137103 0.02862897]\n",
      "[613]: Class 0, probability [0.75428374 0.24571626]\n",
      "[940]: Class 1, probability [0.02703822 0.97296178]\n",
      "[599]: Class 0, probability [0.78968288 0.21031712]\n",
      "[124]: Class 0, probability [9.99713371e-01 2.86628527e-04]\n",
      "[447]: Class 0, probability [0.9709682 0.0290318]\n",
      "[172]: Class 0, probability [9.99428367e-01 5.71633365e-04]\n",
      "[674]: Class 0, probability [0.56068902 0.43931098]\n",
      "[615]: Class 0, probability [0.74891168 0.25108832]\n",
      "[597]: Class 0, probability [0.79442208 0.20557792]\n",
      "[562]: Class 0, probability [0.86475395 0.13524605]\n",
      "[987]: Class 1, probability [0.01393523 0.98606477]\n",
      "[610]: Class 0, probability [0.76219536 0.23780464]\n",
      "[730]: Class 1, probability [0.3631458 0.6368542]\n",
      "[706]: Class 1, probability [0.44610065 0.55389935]\n",
      "[526]: Class 0, probability [0.91476821 0.08523179]\n",
      "[728]: Class 1, probability [0.36982643 0.63017357]\n",
      "[181]: Class 0, probability [9.99349393e-01 6.50606838e-04]\n",
      "[593]: Class 0, probability [0.80366174 0.19633826]\n",
      "[157]: Class 0, probability [9.99539277e-01 4.60723263e-04]\n",
      "[724]: Class 1, probability [0.38333613 0.61666387]\n",
      "[936]: Class 1, probability [0.02859411 0.97140589]\n",
      "[953]: Class 1, probability [0.02252978 0.97747022]\n",
      "[695]: Class 1, probability [0.48545989 0.51454011]\n",
      "[943]: Class 1, probability [0.02592563 0.97407437]\n",
      "[976]: Class 1, probability [0.01628582 0.98371418]\n",
      "[210]: Class 0, probability [9.99012868e-01 9.87131975e-04]\n",
      "[573]: Class 0, probability [0.84515399 0.15484601]\n",
      "[973]: Class 1, probability [0.01699194 0.98300806]\n",
      "[35]: Class 0, probability [9.99920328e-01 7.96716771e-05]\n",
      "[267]: Class 0, probability [0.99776134 0.00223866]\n",
      "[356]: Class 0, probability [0.99199107 0.00800893]\n",
      "[269]: Class 0, probability [0.99769614 0.00230386]\n",
      "[246]: Class 0, probability [0.99834413 0.00165587]\n",
      "[198]: Class 0, probability [9.99169265e-01 8.30735380e-04]\n",
      "[688]: Class 0, probability [0.51063216 0.48936784]\n",
      "[751]: Class 1, probability [0.296531 0.703469]\n",
      "[356]: Class 0, probability [0.99199107 0.00800893]\n",
      "[251]: Class 0, probability [0.99822084 0.00177916]\n",
      "[373]: Class 0, probability [0.9897945 0.0102055]\n",
      "[927]: Class 1, probability [0.03241892 0.96758108]\n",
      "[383]: Class 0, probability [0.9882339 0.0117661]\n",
      "[85]: Class 0, probability [9.99836437e-01 1.63563448e-04]\n",
      "[122]: Class 0, probability [9.99721499e-01 2.78500638e-04]\n",
      "[166]: Class 0, probability [9.99475618e-01 5.24382094e-04]\n",
      "[529]: Class 0, probability [0.91134216 0.08865784]\n",
      "[598]: Class 0, probability [0.79206244 0.20793756]\n",
      "[17]: Class 0, probability [9.99938505e-01 6.14951845e-05]\n",
      "[203]: Class 0, probability [9.99107357e-01 8.92642728e-04]\n",
      "[756]: Class 1, probability [0.28174766 0.71825234]\n",
      "[333]: Class 0, probability [0.99423442 0.00576558]\n",
      "[42]: Class 0, probability [9.99911887e-01 8.81127825e-05]\n",
      "[119]: Class 0, probability [9.99733261e-01 2.66738792e-04]\n",
      "[47]: Class 0, probability [9.99905316e-01 9.46842918e-05]\n",
      "[297]: Class 0, probability [0.99655717 0.00344283]\n",
      "[622]: Class 0, probability [0.72950339 0.27049661]\n",
      "[426]: Class 0, probability [0.97837489 0.02162511]\n",
      "[633]: Class 0, probability [0.69716687 0.30283313]\n",
      "[395]: Class 0, probability [0.98604752 0.01395248]\n",
      "[127]: Class 0, probability [9.99700733e-01 2.99267149e-04]\n",
      "[422]: Class 0, probability [0.97955955 0.02044045]\n",
      "[364]: Class 0, probability [0.99102287 0.00897713]\n",
      "[749]: Class 1, probability [0.30256837 0.69743163]\n",
      "[481]: Class 0, probability [0.9535019 0.0464981]\n",
      "[462]: Class 0, probability [0.96422553 0.03577447]\n",
      "[396]: Class 0, probability [0.98584819 0.01415181]\n",
      "[122]: Class 0, probability [9.99721499e-01 2.78500638e-04]\n",
      "[550]: Class 0, probability [0.88370552 0.11629448]\n",
      "[936]: Class 1, probability [0.02859411 0.97140589]\n",
      "[684]: Class 0, probability [0.52500032 0.47499968]\n",
      "[903]: Class 1, probability [0.04518471 0.95481529]\n",
      "[672]: Class 0, probability [0.5677639 0.4322361]\n",
      "[487]: Class 0, probability [0.94952125 0.05047875]\n",
      "[623]: Class 0, probability [0.72665499 0.27334501]\n",
      "[871]: Class 1, probability [0.0697613 0.9302387]\n",
      "[199]: Class 0, probability [9.99157236e-01 8.42763763e-04]\n",
      "[364]: Class 0, probability [0.99102287 0.00897713]\n",
      "[332]: Class 0, probability [0.99431631 0.00568369]\n",
      "[976]: Class 1, probability [0.01628582 0.98371418]\n",
      "[369]: Class 0, probability [0.99035974 0.00964026]\n",
      "[389]: Class 0, probability [0.98718666 0.01281334]\n",
      "[406]: Class 0, probability [0.98369407 0.01630593]\n",
      "[980]: Class 1, probability [0.01538906 0.98461094]\n",
      "[458]: Class 0, probability [0.96615851 0.03384149]\n",
      "[729]: Class 1, probability [0.3664797 0.6335203]\n",
      "[795]: Class 1, probability [0.1828856 0.8171144]\n",
      "[334]: Class 0, probability [0.99415136 0.00584864]\n",
      "[50]: Class 0, probability [9.99901140e-01 9.88601456e-05]\n",
      "[656]: Class 0, probability [0.62314819 0.37685181]\n",
      "[962]: Class 1, probability [0.01984776 0.98015224]\n",
      "\n",
      "99 out of 100 correct.\n"
     ]
    }
   ],
   "source": [
    "#Finally, output the results, formatted for nicer viewing.\n",
    "#The format is [<sample value>]: Class <class number>, probability [ <probability for class 0> <probability for class 1>]\n",
    "#So, the probability array is the probability of failure, followed by the probability of passing.\n",
    "#In an example run, [7]: Class 0, probability [  9.99966694e-01   3.33062825e-05]\n",
    "#Means that for value 7, the class is 0 (failure) and the probability of failure is 99.9%\n",
    "for i in range(0,100):\n",
    "    if (true_classifier(samples[i])) == (_class[i] == 1):\n",
    "        num_accurate = num_accurate + 1\n",
    "    print(\"\" + str(samples[i]) + \": Class \" + str(_class[i]) + \", probability \" + str(proba[i]))\n",
    "#skip a line to separate overall result from sample output\n",
    "print(\"\")\n",
    "print(str(num_accurate) +\" out of 100 correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
